{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ein LLM lokal starten\n",
    "In diesem Python-Notebook haben wir eine Schritt für Schritt Anleitung, wie du DeepSeek lokal laufen lassen kannst.\\\n",
    "Da ein gutes LLM zu betreiben sehr leistungsintensiv ist, können wir auf unseren PCs nur eine einfache Version betreiben.\n",
    "\n",
    "Wir arbeiten mit einem Python-Notebook (.ipynb Datei). Es gibt Markdown Zellen wie diese hier. Darin gibt es (hoffentlich) nützlichen Text.\\\n",
    "Zusätzlich gibt es auch Codezeilen wie weiter unten, inder man diverse Code abspielen kann.\\\n",
    "Wähle vor dem Arbeiten mit dem Notebook noch den Kernel aus in deinem venv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama installieren\n",
    "Um ein Model lokal starten zu können, brauchen wir [Ollama](https://ollama.com).\\\n",
    "Zunächst musst du Ollama auf deinem System installieren:\n",
    "### MacOS\n",
    "`brew install ollama`\n",
    "\n",
    "### Windows\n",
    "Besuche die [Ollama-Website](https://ollama.com) und lade den Windows-Installer herunter.\n",
    "\n",
    "### Ollama starten\n",
    "Um mit Ollama zu arbeiten muss Ollama gestartet werden. \\\n",
    "Starte Ollama in der Konsole mit `ollama serve`.\\\n",
    "Dieser `serve` Befehl muss in einem separaten Terminal ausgeführt werden, da es das Terminal blockiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Bibliothek installieren\n",
    "Um Ollama mit Python zu verwenden, müssen wir ollama mit pip installieren:\n",
    "Beachte: ein \"!\" in einer Code Zelle führt Kommandozeilen Code aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeiten mit Ollama\n",
    "Nun haben wir Ollama installiert und können aus verschiedenen Models auswählen ([alle](https://ollama.com/search)).\\\n",
    "Wir arbeiten nun mit `deepseek-r1` was wir [hier](https://ollama.com/library/deepseek-r1) finden können.\\\n",
    "\n",
    "### Model herunterladen\n",
    "Das Model können wir mit dem Befehl `ollama pull MODELNAME`herunterladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Wir laden das Model kleinste Model runter, da wir nicht zu viel Platz verbrauchen wollen.\"\"\"\n",
    "! ollama pull deepseek-r1:1.5b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model starten\n",
    "Das heruntergeladene Modell können wir nun starten mit `ollama run deepseek-r1:1.5b`\n",
    "Führe diesen Befehl ebenfalls in einem separatem Terminal aus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model verwenden mit python\n",
    "Nun können wir das Model mit python verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# Einfache Anfrage an das Modell\n",
    "response = ollama.chat(model='deepseek-r1:1.5b', messages=[\n",
    "    {\n",
    "        'role': 'user',\n",
    "        'content': 'Erkläre mir, was Machine Learning ist, in drei Sätzen.'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(response['message']['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
